{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scratch Jupyter Noteboook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import matplotlib.pyplot as plt\n",
    "import nonlinear_approximator as na\n",
    "from importlib import reload\n",
    "import torch\n",
    "import torchvision\n",
    "import PIL\n",
    "%matplotlib inline\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask Client for Distributed Computation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Plot MNIST Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_ten(int_label: torch.Tensor) -> NDArray[np.floating]:\n",
    "    oh = np.zeros((10,))\n",
    "    oh[int_label] = 1\n",
    "    return oh\n",
    "\n",
    "def to_numpy_arr(img: PIL.Image) -> NDArray[np.floating]:\n",
    "    # rescale 255 to +/- 1 \n",
    "    arr = np.asarray(img).flatten()\n",
    "    arr = arr / 255 # 0 --> 1\n",
    "    arr = arr - .5  # -.5 --> .5\n",
    "    arr = 2 * arr   # -1 --> 1\n",
    "    return arr\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST(root='./data', download=True, train=True, transform=to_numpy_arr, target_transform=one_hot_ten)\n",
    "test_data = torchvision.datasets.MNIST(root='./data', download=True, train=False, transform=to_numpy_arr, target_transform=one_hot_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_rows = 2\n",
    "num_cols = 5\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(1.5*num_cols,2*num_rows))\n",
    "for i in range(num_rows*num_cols):\n",
    "    ax = axes[i//num_cols, i%num_cols]\n",
    "    ax.imshow(train_data[i][0].reshape((28, 28)), cmap='gray')\n",
    "    ax.set_title('Label: {}'.format(np.argmax(train_data[i][1])))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(na)\n",
    "reload(na.model)\n",
    "reload(na.activations)\n",
    "reload(na.params)\n",
    "reload(na.training)\n",
    "\n",
    "config = na.params.RegressionParams(\n",
    "    width=100,\n",
    "    depth=50,\n",
    "    input_dimension=len(train_data[0][0]),\n",
    "    transform_type=na.activations.TransformType.TENT,\n",
    "    transform_params=na.params.TentParams(mu=1.99),\n",
    "    output_dimension=len(train_data[0][1]),\n",
    "    batch_size=1000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fritz/untitled/.venv/lib/python3.11/site-packages/distributed/client.py:3363: UserWarning: Sending large graph of size 358.89 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data images with shape (60000, 784), and labels with shape (60000, 10)\n",
      "Loaded test data images with shape (10000, 784), and labels with shape (10000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fritz/untitled/.venv/lib/python3.11/site-packages/distributed/client.py:3363: UserWarning: Sending large graph of size 59.82 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = na.model.NonlinearRegressorModel(config)\n",
    "imgs_train, labels_train = zip(*train_data)\n",
    "imgs_train = da.array(imgs_train).persist()\n",
    "labels_train = da.array(labels_train).persist()\n",
    "\n",
    "\n",
    "imgs_test, labels_test = zip(*test_data)\n",
    "imgs_test = da.array(imgs_test).persist()\n",
    "labels_test = da.array(labels_test).persist()\n",
    "\n",
    "print(f\"Loaded training data images with shape {imgs_train.shape}, and labels with shape {labels_train.shape}\")\n",
    "print(f\"Loaded test data images with shape {imgs_test.shape}, and labels with shape {labels_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Client in module distributed.client object:\n",
      "\n",
      "class Client(distributed.utils.SyncMethodMixin)\n",
      " |  Client(address=None, loop=None, timeout=<no_default>, set_as_default=True, scheduler_file=None, security=None, asynchronous=False, name=None, heartbeat_interval=None, serializers=None, deserializers=None, extensions={}, direct_to_workers=None, connection_limit=512, **kwargs)\n",
      " |  \n",
      " |  Connect to and submit computation to a Dask cluster\n",
      " |  \n",
      " |  The Client connects users to a Dask cluster.  It provides an asynchronous\n",
      " |  user interface around functions and futures.  This class resembles\n",
      " |  executors in ``concurrent.futures`` but also allows ``Future`` objects\n",
      " |  within ``submit/map`` calls.  When a Client is instantiated it takes over\n",
      " |  all ``dask.compute`` and ``dask.persist`` calls by default.\n",
      " |  \n",
      " |  It is also common to create a Client without specifying the scheduler\n",
      " |  address , like ``Client()``.  In this case the Client creates a\n",
      " |  :class:`LocalCluster` in the background and connects to that.  Any extra\n",
      " |  keywords are passed from Client to LocalCluster in this case.  See the\n",
      " |  LocalCluster documentation for more information.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  address: string, or Cluster\n",
      " |      This can be the address of a ``Scheduler`` server like a string\n",
      " |      ``'127.0.0.1:8786'`` or a cluster object like ``LocalCluster()``\n",
      " |  loop\n",
      " |      The event loop\n",
      " |  timeout: int (defaults to configuration ``distributed.comm.timeouts.connect``)\n",
      " |      Timeout duration for initial connection to the scheduler\n",
      " |  set_as_default: bool (True)\n",
      " |      Use this Client as the global dask scheduler\n",
      " |  scheduler_file: string (optional)\n",
      " |      Path to a file with scheduler information if available\n",
      " |  security: Security or bool, optional\n",
      " |      Optional security information. If creating a local cluster can also\n",
      " |      pass in ``True``, in which case temporary self-signed credentials will\n",
      " |      be created automatically.\n",
      " |  asynchronous: bool (False by default)\n",
      " |      Set to True if using this client within async/await functions or within\n",
      " |      Tornado gen.coroutines.  Otherwise this should remain False for normal\n",
      " |      use.\n",
      " |  name: string (optional)\n",
      " |      Gives the client a name that will be included in logs generated on\n",
      " |      the scheduler for matters relating to this client\n",
      " |  heartbeat_interval: int (optional)\n",
      " |      Time in milliseconds between heartbeats to scheduler\n",
      " |  serializers\n",
      " |      Iterable of approaches to use when serializing the object.\n",
      " |      See :ref:`serialization` for more.\n",
      " |  deserializers\n",
      " |      Iterable of approaches to use when deserializing the object.\n",
      " |      See :ref:`serialization` for more.\n",
      " |  extensions : list\n",
      " |      The extensions\n",
      " |  direct_to_workers: bool (optional)\n",
      " |      Whether or not to connect directly to the workers, or to ask\n",
      " |      the scheduler to serve as intermediary.\n",
      " |  connection_limit : int\n",
      " |      The number of open comms to maintain at once in the connection pool\n",
      " |  \n",
      " |  **kwargs:\n",
      " |      If you do not pass a scheduler address, Client will create a\n",
      " |      ``LocalCluster`` object, passing any extra keyword arguments.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  Provide cluster's scheduler node address on initialization:\n",
      " |  \n",
      " |  >>> client = Client('127.0.0.1:8786')  # doctest: +SKIP\n",
      " |  \n",
      " |  Use ``submit`` method to send individual computations to the cluster\n",
      " |  \n",
      " |  >>> a = client.submit(add, 1, 2)  # doctest: +SKIP\n",
      " |  >>> b = client.submit(add, 10, 20)  # doctest: +SKIP\n",
      " |  \n",
      " |  Continue using submit or map on results to build up larger computations\n",
      " |  \n",
      " |  >>> c = client.submit(add, a, b)  # doctest: +SKIP\n",
      " |  \n",
      " |  Gather results with the ``gather`` method.\n",
      " |  \n",
      " |  >>> client.gather(c)  # doctest: +SKIP\n",
      " |  33\n",
      " |  \n",
      " |  You can also call Client with no arguments in order to create your own\n",
      " |  local cluster.\n",
      " |  \n",
      " |  >>> client = Client()  # makes your own local \"cluster\" # doctest: +SKIP\n",
      " |  \n",
      " |  Extra keywords will be passed directly to LocalCluster\n",
      " |  \n",
      " |  >>> client = Client(n_workers=2, threads_per_worker=4)  # doctest: +SKIP\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  distributed.scheduler.Scheduler: Internal scheduler\n",
      " |  distributed.LocalCluster:\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Client\n",
      " |      distributed.utils.SyncMethodMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  async __aenter__(self)\n",
      " |  \n",
      " |  async __aexit__(self, exc_type, exc_value, traceback)\n",
      " |  \n",
      " |  __await__(self)\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_value, traceback)\n",
      " |  \n",
      " |  __init__(self, address=None, loop=None, timeout=<no_default>, set_as_default=True, scheduler_file=None, security=None, asynchronous=False, name=None, heartbeat_interval=None, serializers=None, deserializers=None, extensions={}, direct_to_workers=None, connection_limit=512, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  as_current(self)\n",
      " |      Thread-local, Task-local context manager that causes the Client.current\n",
      " |      class method to return self. Any Future objects deserialized inside this\n",
      " |      context manager will be automatically attached to this Client.\n",
      " |  \n",
      " |  benchmark_hardware(self) -> 'dict'\n",
      " |      Run a benchmark on the workers for memory, disk, and network bandwidths\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result: dict\n",
      " |          A dictionary mapping the names \"disk\", \"memory\", and \"network\" to\n",
      " |          dictionaries mapping sizes to bandwidths.  These bandwidths are\n",
      " |          averaged over many workers running computations across the cluster.\n",
      " |  \n",
      " |  call_stack(self, futures=None, keys=None)\n",
      " |      The actively running call stack of all relevant keys\n",
      " |      \n",
      " |      You can specify data of interest either by providing futures or\n",
      " |      collections in the ``futures=`` keyword or a list of explicit keys in\n",
      " |      the ``keys=`` keyword.  If neither are provided then all call stacks\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      futures : list (optional)\n",
      " |          List of futures, defaults to all data\n",
      " |      keys : list (optional)\n",
      " |          List of key names, defaults to all data\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = dd.read_parquet(...).persist()  # doctest: +SKIP\n",
      " |      >>> client.call_stack(df)  # call on collections\n",
      " |      \n",
      " |      >>> client.call_stack()  # Or call with no arguments for all activity  # doctest: +SKIP\n",
      " |  \n",
      " |  cancel(self, futures, asynchronous=None, force=False, reason=None, msg=None)\n",
      " |      Cancel running futures\n",
      " |      This stops future tasks from being scheduled if they have not yet run\n",
      " |      and deletes them if they have already run.  After calling, this result\n",
      " |      and all dependent results will no longer be accessible\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      futures : List[Future]\n",
      " |          The list of Futures\n",
      " |      asynchronous: bool\n",
      " |          If True the client is in asynchronous mode\n",
      " |      force : boolean (False)\n",
      " |          Cancel this future even if other clients desire it\n",
      " |      reason: str\n",
      " |          Reason for cancelling the futures\n",
      " |      msg : str\n",
      " |          Message that will be attached to the cancelled future\n",
      " |  \n",
      " |  close(self, timeout=<no_default>)\n",
      " |      Close this client\n",
      " |      \n",
      " |      Clients will also close automatically when your Python session ends\n",
      " |      \n",
      " |      If you started a client without arguments like ``Client()`` then this\n",
      " |      will also close the local cluster that was started at the same time.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      timeout : number\n",
      " |          Time in seconds after which to raise a\n",
      " |          ``dask.distributed.TimeoutError``\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.restart\n",
      " |  \n",
      " |  compute(self, collections, sync=False, optimize_graph=True, workers=None, allow_other_workers=False, resources=None, retries=0, priority=0, fifo_timeout='60s', actors=None, traverse=True, **kwargs)\n",
      " |      Compute dask collections on cluster\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      collections : iterable of dask objects or single dask object\n",
      " |          Collections like dask.array or dataframe or dask.value objects\n",
      " |      sync : bool (optional)\n",
      " |          Returns Futures if False (default) or concrete values if True\n",
      " |      optimize_graph : bool\n",
      " |          Whether or not to optimize the underlying graphs\n",
      " |      workers : string or iterable of strings\n",
      " |          A set of worker hostnames on which computations may be performed.\n",
      " |          Leave empty to default to all workers (common case)\n",
      " |      allow_other_workers : bool (defaults to False)\n",
      " |          Used with `workers`. Indicates whether or not the computations\n",
      " |          may be performed on workers that are not in the `workers` set(s).\n",
      " |      retries : int (default to 0)\n",
      " |          Number of allowed automatic retries if computing a result fails\n",
      " |      priority : Number\n",
      " |          Optional prioritization of task.  Zero is default.\n",
      " |          Higher priorities take precedence\n",
      " |      fifo_timeout : timedelta str (defaults to '60s')\n",
      " |          Allowed amount of time between calls to consider the same priority\n",
      " |      traverse : bool (defaults to True)\n",
      " |          By default dask traverses builtin python collections looking for\n",
      " |          dask objects passed to ``compute``. For large collections this can\n",
      " |          be expensive. If none of the arguments contain any dask objects,\n",
      " |          set ``traverse=False`` to avoid doing this traversal.\n",
      " |      resources : dict (defaults to {})\n",
      " |          Defines the `resources` each instance of this mapped task requires\n",
      " |          on the worker; e.g. ``{'GPU': 2}``.\n",
      " |          See :doc:`worker resources <resources>` for details on defining\n",
      " |          resources.\n",
      " |      actors : bool or dict (default None)\n",
      " |          Whether these tasks should exist on the worker as stateful actors.\n",
      " |          Specified on a global (True/False) or per-task (``{'x': True,\n",
      " |          'y': False}``) basis. See :doc:`actors` for additional details.\n",
      " |      **kwargs\n",
      " |          Options to pass to the graph optimize calls\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List of Futures if input is a sequence, or a single future otherwise\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from dask import delayed\n",
      " |      >>> from operator import add\n",
      " |      >>> x = delayed(add)(1, 2)\n",
      " |      >>> y = delayed(add)(x, x)\n",
      " |      >>> xx, yy = client.compute([x, y])  # doctest: +SKIP\n",
      " |      >>> xx  # doctest: +SKIP\n",
      " |      <Future: status: finished, key: add-8f6e709446674bad78ea8aeecfee188e>\n",
      " |      >>> xx.result()  # doctest: +SKIP\n",
      " |      3\n",
      " |      >>> yy.result()  # doctest: +SKIP\n",
      " |      6\n",
      " |      \n",
      " |      Also support single arguments\n",
      " |      \n",
      " |      >>> xx = client.compute(x)  # doctest: +SKIP\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.get : Normal synchronous dask.get function\n",
      " |  \n",
      " |  dump_cluster_state(self, filename: 'str' = 'dask-cluster-dump', write_from_scheduler: 'bool | None' = None, exclude: 'Collection[str]' = (), format: \"Literal['msgpack', 'yaml']\" = 'msgpack', **storage_options)\n",
      " |      Extract a dump of the entire cluster state and persist to disk or a URL.\n",
      " |      This is intended for debugging purposes only.\n",
      " |      \n",
      " |      Warning: Memory usage on the scheduler (and client, if writing the dump locally)\n",
      " |      can be large. On a large or long-running cluster, this can take several minutes.\n",
      " |      The scheduler may be unresponsive while the dump is processed.\n",
      " |      \n",
      " |      Results will be stored in a dict::\n",
      " |      \n",
      " |          {\n",
      " |              \"scheduler\": {...},  # scheduler state\n",
      " |              \"workers\": {\n",
      " |                  worker_addr: {...},  # worker state\n",
      " |                  ...\n",
      " |              }\n",
      " |              \"versions\": {\n",
      " |                  \"scheduler\": {...},\n",
      " |                  \"workers\": {\n",
      " |                      worker_addr: {...},\n",
      " |                      ...\n",
      " |                  }\n",
      " |              }\n",
      " |          }\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      filename:\n",
      " |          The path or URL to write to. The appropriate file suffix (``.msgpack.gz`` or\n",
      " |          ``.yaml``) will be appended automatically.\n",
      " |      \n",
      " |          Must be a path supported by :func:`fsspec.open` (like ``s3://my-bucket/cluster-dump``,\n",
      " |          or ``cluster-dumps/dump``). See ``write_from_scheduler`` to control whether\n",
      " |          the dump is written directly to ``filename`` from the scheduler, or sent\n",
      " |          back to the client over the network, then written locally.\n",
      " |      write_from_scheduler:\n",
      " |          If None (default), infer based on whether ``filename`` looks like a URL\n",
      " |          or a local path: True if the filename contains ``://`` (like\n",
      " |          ``s3://my-bucket/cluster-dump``), False otherwise (like ``local_dir/cluster-dump``).\n",
      " |      \n",
      " |          If True, write cluster state directly to ``filename`` from the scheduler.\n",
      " |          If ``filename`` is a local path, the dump will be written to that\n",
      " |          path on the *scheduler's* filesystem, so be careful if the scheduler is running\n",
      " |          on ephemeral hardware. Useful when the scheduler is attached to a network\n",
      " |          filesystem or persistent disk, or for writing to buckets.\n",
      " |      \n",
      " |          If False, transfer cluster state from the scheduler back to the client\n",
      " |          over the network, then write it to ``filename``. This is much less\n",
      " |          efficient for large dumps, but useful when the scheduler doesn't have\n",
      " |          access to any persistent storage.\n",
      " |      exclude:\n",
      " |          A collection of attribute names which are supposed to be excluded\n",
      " |          from the dump, e.g. to exclude code, tracebacks, logs, etc.\n",
      " |      \n",
      " |          Defaults to exclude ``run_spec``, which is the serialized user code.\n",
      " |          This is typically not required for debugging. To allow serialization\n",
      " |          of this, pass an empty tuple.\n",
      " |      format:\n",
      " |          Either ``\"msgpack\"`` or ``\"yaml\"``. If msgpack is used (default),\n",
      " |          the output will be stored in a gzipped file as msgpack.\n",
      " |      \n",
      " |          To read::\n",
      " |      \n",
      " |              import gzip, msgpack\n",
      " |              with gzip.open(\"filename\") as fd:\n",
      " |                  state = msgpack.unpack(fd)\n",
      " |      \n",
      " |          or::\n",
      " |      \n",
      " |              import yaml\n",
      " |              try:\n",
      " |                  from yaml import CLoader as Loader\n",
      " |              except ImportError:\n",
      " |                  from yaml import Loader\n",
      " |              with open(\"filename\") as fd:\n",
      " |                  state = yaml.load(fd, Loader=Loader)\n",
      " |      **storage_options:\n",
      " |          Any additional arguments to :func:`fsspec.open` when writing to a URL.\n",
      " |  \n",
      " |  forward_logging(self, logger_name=None, level=0)\n",
      " |      Begin forwarding the given logger (by default the root) and all loggers\n",
      " |      under it from worker tasks to the client process. Whenever the named\n",
      " |      logger handles a LogRecord on the worker-side, the record will be\n",
      " |      serialized, sent to the client, and handled by the logger with the same\n",
      " |      name on the client-side.\n",
      " |      \n",
      " |      Note that worker-side loggers will only handle LogRecords if their level\n",
      " |      is set appropriately, and the client-side logger will only emit the\n",
      " |      forwarded LogRecord if its own level is likewise set appropriately. For\n",
      " |      example, if your submitted task logs a DEBUG message to logger \"foo\",\n",
      " |      then in order for ``forward_logging()`` to cause that message to be\n",
      " |      emitted in your client session, you must ensure that the logger \"foo\"\n",
      " |      have its level set to DEBUG (or lower) in the worker process *and* in the\n",
      " |      client process.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      logger_name : str, optional\n",
      " |          The name of the logger to begin forwarding. The usual rules of the\n",
      " |          ``logging`` module's hierarchical naming system apply. For example,\n",
      " |          if ``name`` is ``\"foo\"``, then not only ``\"foo\"``, but also\n",
      " |          ``\"foo.bar\"``, ``\"foo.baz\"``, etc. will be forwarded. If ``name`` is\n",
      " |          ``None``, this indicates the root logger, and so *all* loggers will\n",
      " |          be forwarded.\n",
      " |      \n",
      " |          Note that a logger will only forward a given LogRecord if the\n",
      " |          logger's level is sufficient for the LogRecord to be handled at all.\n",
      " |      \n",
      " |      level : str | int, optional\n",
      " |          Optionally restrict forwarding to LogRecords of this level or\n",
      " |          higher, even if the forwarded logger's own level is lower.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      For purposes of the examples, suppose we configure client-side logging\n",
      " |      as a user might: with a single StreamHandler attached to the root logger\n",
      " |      with an output level of INFO and a simple output format::\n",
      " |      \n",
      " |          import logging\n",
      " |          import distributed\n",
      " |          import io, yaml\n",
      " |      \n",
      " |          TYPICAL_LOGGING_CONFIG = '''\n",
      " |          version: 1\n",
      " |          handlers:\n",
      " |            console:\n",
      " |              class : logging.StreamHandler\n",
      " |              formatter: default\n",
      " |              level   : INFO\n",
      " |          formatters:\n",
      " |            default:\n",
      " |              format: '%(asctime)s %(levelname)-8s [worker %(worker)s] %(name)-15s %(message)s'\n",
      " |              datefmt: '%Y-%m-%d %H:%M:%S'\n",
      " |          root:\n",
      " |            handlers:\n",
      " |              - console\n",
      " |          '''\n",
      " |          config = yaml.safe_load(io.StringIO(TYPICAL_LOGGING_CONFIG))\n",
      " |          logging.config.dictConfig(config)\n",
      " |      \n",
      " |      Now create a client and begin forwarding the root logger from workers\n",
      " |      back to our local client process.\n",
      " |      \n",
      " |      >>> client = distributed.Client()\n",
      " |      >>> client.forward_logging()  # forward the root logger at any handled level\n",
      " |      \n",
      " |      Then submit a task that does some error logging on a worker. We see\n",
      " |      output from the client-side StreamHandler.\n",
      " |      \n",
      " |      >>> def do_error():\n",
      " |      ...     logging.getLogger(\"user.module\").error(\"Hello error\")\n",
      " |      ...     return 42\n",
      " |      >>> client.submit(do_error).result()\n",
      " |      2022-11-09 03:43:25 ERROR    [worker tcp://127.0.0.1:34783] user.module     Hello error\n",
      " |      42\n",
      " |      \n",
      " |      Note how an attribute ``\"worker\"`` is also added by dask to the\n",
      " |      forwarded LogRecord, which our custom formatter uses. This is useful for\n",
      " |      identifying exactly which worker logged the error.\n",
      " |      \n",
      " |      One nuance worth highlighting: even though our client-side root logger\n",
      " |      is configured with a level of INFO, the worker-side root loggers still\n",
      " |      have their default level of ERROR because we haven't done any explicit\n",
      " |      logging configuration on the workers. Therefore worker-side INFO logs\n",
      " |      will *not* be forwarded because they never even get handled in the first\n",
      " |      place.\n",
      " |      \n",
      " |      >>> def do_info_1():\n",
      " |      ...     # no output on the client side\n",
      " |      ...     logging.getLogger(\"user.module\").info(\"Hello info the first time\")\n",
      " |      ...     return 84\n",
      " |      >>> client.submit(do_info_1).result()\n",
      " |      84\n",
      " |      \n",
      " |      It is necessary to set the client-side logger's level to INFO before the info\n",
      " |      message will be handled and forwarded to the client. In other words, the\n",
      " |      \"effective\" level of the client-side forwarded logging is the maximum of each\n",
      " |      logger's client-side and worker-side levels.\n",
      " |      \n",
      " |      >>> def do_info_2():\n",
      " |      ...     logger = logging.getLogger(\"user.module\")\n",
      " |      ...     logger.setLevel(logging.INFO)\n",
      " |      ...     # now produces output on the client side\n",
      " |      ...     logger.info(\"Hello info the second time\")\n",
      " |      ...     return 84\n",
      " |      >>> client.submit(do_info_2).result()\n",
      " |      2022-11-09 03:57:39 INFO     [worker tcp://127.0.0.1:42815] user.module     Hello info the second time\n",
      " |      84\n",
      " |  \n",
      " |  futures_of(self, futures)\n",
      " |      Wrapper method of futures_of\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      futures : tuple\n",
      " |          The futures\n",
      " |  \n",
      " |  gather(self, futures, errors='raise', direct=None, asynchronous=None)\n",
      " |      Gather futures from distributed memory\n",
      " |      \n",
      " |      Accepts a future, nested container of futures, iterator, or queue.\n",
      " |      The return type will match the input type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      futures : Collection of futures\n",
      " |          This can be a possibly nested collection of Future objects.\n",
      " |          Collections can be lists, sets, or dictionaries\n",
      " |      errors : string\n",
      " |          Either 'raise' or 'skip' if we should raise if a future has erred\n",
      " |          or skip its inclusion in the output collection\n",
      " |      direct : boolean\n",
      " |          Whether or not to connect directly to the workers, or to ask\n",
      " |          the scheduler to serve as intermediary.  This can also be set when\n",
      " |          creating the Client.\n",
      " |      asynchronous: bool\n",
      " |          If True the client is in asynchronous mode\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      results: a collection of the same type as the input, but now with\n",
      " |      gathered results rather than futures\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add  # doctest: +SKIP\n",
      " |      >>> c = Client('127.0.0.1:8787')  # doctest: +SKIP\n",
      " |      >>> x = c.submit(add, 1, 2)  # doctest: +SKIP\n",
      " |      >>> c.gather(x)  # doctest: +SKIP\n",
      " |      3\n",
      " |      >>> c.gather([x, [x], x])  # support lists and dicts # doctest: +SKIP\n",
      " |      [3, [3], 3]\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.scatter : Send data out to cluster\n",
      " |  \n",
      " |  get(self, dsk, keys, workers=None, allow_other_workers=None, resources=None, sync=True, asynchronous=None, direct=None, retries=None, priority=0, fifo_timeout='60s', actors=None, **kwargs)\n",
      " |      Compute dask graph\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dsk : dict\n",
      " |      keys : object, or nested lists of objects\n",
      " |      workers : string or iterable of strings\n",
      " |          A set of worker addresses or hostnames on which computations may be\n",
      " |          performed. Leave empty to default to all workers (common case)\n",
      " |      allow_other_workers : bool (defaults to False)\n",
      " |          Used with ``workers``. Indicates whether or not the computations\n",
      " |          may be performed on workers that are not in the `workers` set(s).\n",
      " |      resources : dict (defaults to {})\n",
      " |          Defines the ``resources`` each instance of this mapped task\n",
      " |          requires on the worker; e.g. ``{'GPU': 2}``.\n",
      " |          See :doc:`worker resources <resources>` for details on defining\n",
      " |          resources.\n",
      " |      sync : bool (optional)\n",
      " |          Returns Futures if False or concrete values if True (default).\n",
      " |      asynchronous: bool\n",
      " |          If True the client is in asynchronous mode\n",
      " |      direct : bool\n",
      " |          Whether or not to connect directly to the workers, or to ask\n",
      " |          the scheduler to serve as intermediary.  This can also be set when\n",
      " |          creating the Client.\n",
      " |      retries : int (default to 0)\n",
      " |          Number of allowed automatic retries if computing a result fails\n",
      " |      priority : Number\n",
      " |          Optional prioritization of task.  Zero is default.\n",
      " |          Higher priorities take precedence\n",
      " |      fifo_timeout : timedelta str (defaults to '60s')\n",
      " |          Allowed amount of time between calls to consider the same priority\n",
      " |      actors : bool or dict (default None)\n",
      " |          Whether these tasks should exist on the worker as stateful actors.\n",
      " |          Specified on a global (True/False) or per-task (``{'x': True,\n",
      " |          'y': False}``) basis. See :doc:`actors` for additional details.\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      results\n",
      " |          If 'sync' is True, returns the results. Otherwise, returns the\n",
      " |          known data packed\n",
      " |          If 'sync' is False, returns the known data. Otherwise, returns\n",
      " |          the results\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add  # doctest: +SKIP\n",
      " |      >>> c = Client('127.0.0.1:8787')  # doctest: +SKIP\n",
      " |      >>> c.get({'x': (add, 1, 2)}, 'x')  # doctest: +SKIP\n",
      " |      3\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.compute : Compute asynchronous collections\n",
      " |  \n",
      " |  get_dataset(self, name, default=<no_default>, **kwargs)\n",
      " |      Get named dataset from the scheduler if present.\n",
      " |      Return the default or raise a KeyError if not present.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          name of the dataset to retrieve\n",
      " |      default : str\n",
      " |          optional, not set by default\n",
      " |          If set, do not raise a KeyError if the name is not present but\n",
      " |          return this default\n",
      " |      kwargs : dict\n",
      " |          additional keyword arguments to _get_dataset\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      The dataset from the scheduler, if present\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.publish_dataset\n",
      " |      Client.list_datasets\n",
      " |  \n",
      " |  get_events(self, topic: 'str | None' = None)\n",
      " |      Retrieve structured topic logs\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topic : str, optional\n",
      " |          Name of topic log to retrieve events for. If no ``topic`` is\n",
      " |          provided, then logs for all topics will be returned.\n",
      " |  \n",
      " |  get_executor(self, **kwargs)\n",
      " |      Return a concurrent.futures Executor for submitting tasks on this\n",
      " |      Client\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs\n",
      " |          Any submit()- or map()- compatible arguments, such as\n",
      " |          `workers` or `resources`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ClientExecutor\n",
      " |          An Executor object that's fully compatible with the\n",
      " |          concurrent.futures API.\n",
      " |  \n",
      " |  get_metadata(self, keys, default=<no_default>)\n",
      " |      Get arbitrary metadata from scheduler\n",
      " |      \n",
      " |      See set_metadata for the full docstring with examples\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : key or list\n",
      " |          Key to access.  If a list then gets within a nested collection\n",
      " |      default : optional\n",
      " |          If the key does not exist then return this value instead.\n",
      " |          If not provided then this raises a KeyError if the key is not\n",
      " |          present\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.set_metadata\n",
      " |  \n",
      " |  get_scheduler_logs(self, n=None)\n",
      " |      Get logs from scheduler\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int\n",
      " |          Number of logs to retrieve.  Maxes out at 10000 by default,\n",
      " |          configurable via the ``distributed.admin.log-length``\n",
      " |          configuration value.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Logs in reversed order (newest first)\n",
      " |  \n",
      " |  get_task_stream(self, start=None, stop=None, count=None, plot=False, filename='task-stream.html', bokeh_resources=None)\n",
      " |      Get task stream data from scheduler\n",
      " |      \n",
      " |      This collects the data present in the diagnostic \"Task Stream\" plot on\n",
      " |      the dashboard.  It includes the start, stop, transfer, and\n",
      " |      deserialization time of every task for a particular duration.\n",
      " |      \n",
      " |      Note that the task stream diagnostic does not run by default.  You may\n",
      " |      wish to call this function once before you start work to ensure that\n",
      " |      things start recording, and then again after you have completed.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start : Number or string\n",
      " |          When you want to start recording\n",
      " |          If a number it should be the result of calling time()\n",
      " |          If a string then it should be a time difference before now,\n",
      " |          like '60s' or '500 ms'\n",
      " |      stop : Number or string\n",
      " |          When you want to stop recording\n",
      " |      count : int\n",
      " |          The number of desired records, ignored if both start and stop are\n",
      " |          specified\n",
      " |      plot : boolean, str\n",
      " |          If true then also return a Bokeh figure\n",
      " |          If plot == 'save' then save the figure to a file\n",
      " |      filename : str (optional)\n",
      " |          The filename to save to if you set ``plot='save'``\n",
      " |      bokeh_resources : bokeh.resources.Resources (optional)\n",
      " |          Specifies if the resource component is INLINE or CDN\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> client.get_task_stream()  # prime plugin if not already connected\n",
      " |      >>> x.compute()  # do some work\n",
      " |      >>> client.get_task_stream()\n",
      " |      [{'task': ...,\n",
      " |        'type': ...,\n",
      " |        'thread': ...,\n",
      " |        ...}]\n",
      " |      \n",
      " |      Pass the ``plot=True`` or ``plot='save'`` keywords to get back a Bokeh\n",
      " |      figure\n",
      " |      \n",
      " |      >>> data, figure = client.get_task_stream(plot='save', filename='myfile.html')\n",
      " |      \n",
      " |      Alternatively consider the context manager\n",
      " |      \n",
      " |      >>> from dask.distributed import get_task_stream\n",
      " |      >>> with get_task_stream() as ts:\n",
      " |      ...     x.compute()\n",
      " |      >>> ts.data\n",
      " |      [...]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      L: List[Dict]\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      get_task_stream : a context manager version of this method\n",
      " |  \n",
      " |  get_versions(self, check: 'bool' = False, packages: 'Sequence[str] | None' = None) -> 'VersionsDict | Coroutine[Any, Any, VersionsDict]'\n",
      " |      Return version info for the scheduler, all workers and myself\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      check\n",
      " |          raise ValueError if all required & optional packages\n",
      " |          do not match\n",
      " |      packages\n",
      " |          Extra package names to check\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> c.get_versions()  # doctest: +SKIP\n",
      " |      \n",
      " |      >>> c.get_versions(packages=['sklearn', 'geopandas'])  # doctest: +SKIP\n",
      " |  \n",
      " |  get_worker_logs(self, n=None, workers=None, nanny=False)\n",
      " |      Get logs from workers\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int\n",
      " |          Number of logs to retrieve.  Maxes out at 10000 by default,\n",
      " |          configurable via the ``distributed.admin.log-length``\n",
      " |          configuration value.\n",
      " |      workers : iterable\n",
      " |          List of worker addresses to retrieve.  Gets all workers by default.\n",
      " |      nanny : bool, default False\n",
      " |          Whether to get the logs from the workers (False) or the nannies\n",
      " |          (True). If specified, the addresses in `workers` should still be\n",
      " |          the worker addresses, not the nanny addresses.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dictionary mapping worker address to logs.\n",
      " |      Logs are returned in reversed order (newest first)\n",
      " |  \n",
      " |  has_what(self, workers=None, **kwargs)\n",
      " |      Which keys are held by which workers\n",
      " |      \n",
      " |      This returns the keys of the data that are held in each worker's\n",
      " |      memory.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      workers : list (optional)\n",
      " |          A list of worker addresses, defaults to all\n",
      " |      **kwargs : dict\n",
      " |          Optional keyword arguments for the remote function\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x, y, z = c.map(inc, [1, 2, 3])  # doctest: +SKIP\n",
      " |      >>> wait([x, y, z])  # doctest: +SKIP\n",
      " |      >>> c.has_what()  # doctest: +SKIP\n",
      " |      {'192.168.1.141:46784': ['inc-1c8dd6be1c21646c71f76c16d09304ea',\n",
      " |                               'inc-fd65c238a7ea60f6a01bf4c8a5fcf44b',\n",
      " |                               'inc-1e297fc27658d7b67b3a758f16bcf47a']}\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.who_has\n",
      " |      Client.nthreads\n",
      " |      Client.processing\n",
      " |  \n",
      " |  list_datasets(self, **kwargs)\n",
      " |      List named datasets available on the scheduler\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.publish_dataset\n",
      " |      Client.get_dataset\n",
      " |  \n",
      " |  log_event(self, topic: 'str | Collection[str]', msg: 'Any')\n",
      " |      Log an event under a given topic\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topic : str, list[str]\n",
      " |          Name of the topic under which to log an event. To log the same\n",
      " |          event under multiple topics, pass a list of topic names.\n",
      " |      msg\n",
      " |          Event message to log. Note this must be msgpack serializable.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from time import time\n",
      " |      >>> client.log_event(\"current-time\", time())\n",
      " |  \n",
      " |  map(self, func: 'Callable', *iterables: 'Collection', key: 'str | list | None' = None, workers: 'str | Iterable[str] | None' = None, retries: 'int | None' = None, resources: 'dict[str, Any] | None' = None, priority: 'int' = 0, allow_other_workers: 'bool' = False, fifo_timeout: 'str' = '100 ms', actor: 'bool' = False, actors: 'bool' = False, pure: 'bool' = True, batch_size=None, **kwargs)\n",
      " |      Map a function on a sequence of arguments\n",
      " |      \n",
      " |      Arguments can be normal objects or Futures\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : callable\n",
      " |          Callable to be scheduled for execution. If ``func`` returns a coroutine, it\n",
      " |          will be run on the main event loop of a worker. Otherwise ``func`` will be\n",
      " |          run in a worker's task executor pool (see ``Worker.executors`` for more\n",
      " |          information.)\n",
      " |      iterables : Iterables\n",
      " |          List-like objects to map over.  They should have the same length.\n",
      " |      key : str, list\n",
      " |          Prefix for task names if string.  Explicit names if list.\n",
      " |      workers : string or iterable of strings\n",
      " |          A set of worker hostnames on which computations may be performed.\n",
      " |          Leave empty to default to all workers (common case)\n",
      " |      retries : int (default to 0)\n",
      " |          Number of allowed automatic retries if a task fails\n",
      " |      resources : dict (defaults to {})\n",
      " |          Defines the `resources` each instance of this mapped task requires\n",
      " |          on the worker; e.g. ``{'GPU': 2}``.\n",
      " |          See :doc:`worker resources <resources>` for details on defining\n",
      " |          resources.\n",
      " |      priority : Number\n",
      " |          Optional prioritization of task.  Zero is default.\n",
      " |          Higher priorities take precedence\n",
      " |      allow_other_workers : bool (defaults to False)\n",
      " |          Used with `workers`. Indicates whether or not the computations\n",
      " |          may be performed on workers that are not in the `workers` set(s).\n",
      " |      fifo_timeout : str timedelta (default '100ms')\n",
      " |          Allowed amount of time between calls to consider the same priority\n",
      " |      actor : bool (default False)\n",
      " |          Whether these tasks should exist on the worker as stateful actors.\n",
      " |          See :doc:`actors` for additional details.\n",
      " |      actors : bool (default False)\n",
      " |          Alias for `actor`\n",
      " |      pure : bool (defaults to True)\n",
      " |          Whether or not the function is pure.  Set ``pure=False`` for\n",
      " |          impure functions like ``np.random.random``. Note that if both\n",
      " |          ``actor`` and ``pure`` kwargs are set to True, then the value\n",
      " |          of ``pure`` will be reverted to False, since an actor is stateful.\n",
      " |          See :ref:`pure functions` for more details.\n",
      " |      batch_size : int, optional (default: just one batch whose size is the entire iterable)\n",
      " |          Submit tasks to the scheduler in batches of (at most)\n",
      " |          ``batch_size``.\n",
      " |          The tradeoff in batch size is that large batches avoid more per-batch overhead,\n",
      " |          but batches that are too big can take a long time to submit and unreasonably delay\n",
      " |          the cluster from starting its processing.\n",
      " |      **kwargs : dict\n",
      " |          Extra keyword arguments to send to the function.\n",
      " |          Large values will be included explicitly in the task graph.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> L = client.map(func, sequence)  # doctest: +SKIP\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The current implementation of a task graph resolution searches for occurrences of ``key``\n",
      " |      and replaces it with a corresponding ``Future`` result. That can lead to unwanted\n",
      " |      substitution of strings passed as arguments to a task if these strings match some ``key``\n",
      " |      that already exists on a cluster. To avoid these situations it is required to use unique\n",
      " |      values if a ``key`` is set manually.\n",
      " |      See https://github.com/dask/dask/issues/9969 to track progress on resolving this issue.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List, iterator, or Queue of futures, depending on the type of the\n",
      " |      inputs.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.submit : Submit a single function\n",
      " |  \n",
      " |  nbytes(self, keys=None, summary=True, **kwargs)\n",
      " |      The bytes taken up by each key on the cluster\n",
      " |      \n",
      " |      This is as measured by ``sys.getsizeof`` which may not accurately\n",
      " |      reflect the true cost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : list (optional)\n",
      " |          A list of keys, defaults to all keys\n",
      " |      summary : boolean, (optional)\n",
      " |          Summarize keys into key types\n",
      " |      **kwargs : dict\n",
      " |          Optional keyword arguments for the remote function\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x, y, z = c.map(inc, [1, 2, 3])  # doctest: +SKIP\n",
      " |      >>> c.nbytes(summary=False)  # doctest: +SKIP\n",
      " |      {'inc-1c8dd6be1c21646c71f76c16d09304ea': 28,\n",
      " |       'inc-1e297fc27658d7b67b3a758f16bcf47a': 28,\n",
      " |       'inc-fd65c238a7ea60f6a01bf4c8a5fcf44b': 28}\n",
      " |      \n",
      " |      >>> c.nbytes(summary=True)  # doctest: +SKIP\n",
      " |      {'inc': 84}\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.who_has\n",
      " |  \n",
      " |  ncores = nthreads(self, workers=None, **kwargs)\n",
      " |  \n",
      " |  normalize_collection(self, collection)\n",
      " |      Replace collection's tasks by already existing futures if they exist\n",
      " |      \n",
      " |      This normalizes the tasks within a collections task graph against the\n",
      " |      known futures within the scheduler.  It returns a copy of the\n",
      " |      collection with a task graph that includes the overlapping futures.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      collection : dask object\n",
      " |          Collection like dask.array or dataframe or dask.value objects\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      collection : dask object\n",
      " |          Collection with its tasks replaced with any existing futures.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> len(x.__dask_graph__())  # x is a dask collection with 100 tasks  # doctest: +SKIP\n",
      " |      100\n",
      " |      >>> set(client.futures).intersection(x.__dask_graph__())  # some overlap exists  # doctest: +SKIP\n",
      " |      10\n",
      " |      \n",
      " |      >>> x = client.normalize_collection(x)  # doctest: +SKIP\n",
      " |      >>> len(x.__dask_graph__())  # smaller computational graph  # doctest: +SKIP\n",
      " |      20\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.persist : trigger computation of collection's tasks\n",
      " |  \n",
      " |  nthreads(self, workers=None, **kwargs)\n",
      " |      The number of threads/cores available on each worker node\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      workers : list (optional)\n",
      " |          A list of workers that we care about specifically.\n",
      " |          Leave empty to receive information about all workers.\n",
      " |      **kwargs : dict\n",
      " |          Optional keyword arguments for the remote function\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> c.nthreads()  # doctest: +SKIP\n",
      " |      {'192.168.1.141:46784': 8,\n",
      " |       '192.167.1.142:47548': 8,\n",
      " |       '192.167.1.143:47329': 8,\n",
      " |       '192.167.1.144:37297': 8}\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.who_has\n",
      " |      Client.has_what\n",
      " |  \n",
      " |  persist(self, collections, optimize_graph=True, workers=None, allow_other_workers=None, resources=None, retries=None, priority=0, fifo_timeout='60s', actors=None, **kwargs)\n",
      " |      Persist dask collections on cluster\n",
      " |      \n",
      " |      Starts computation of the collection on the cluster in the background.\n",
      " |      Provides a new dask collection that is semantically identical to the\n",
      " |      previous one, but now based off of futures currently in execution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      collections : sequence or single dask object\n",
      " |          Collections like dask.array or dataframe or dask.value objects\n",
      " |      optimize_graph : bool\n",
      " |          Whether or not to optimize the underlying graphs\n",
      " |      workers : string or iterable of strings\n",
      " |          A set of worker hostnames on which computations may be performed.\n",
      " |          Leave empty to default to all workers (common case)\n",
      " |      allow_other_workers : bool (defaults to False)\n",
      " |          Used with `workers`. Indicates whether or not the computations\n",
      " |          may be performed on workers that are not in the `workers` set(s).\n",
      " |      retries : int (default to 0)\n",
      " |          Number of allowed automatic retries if computing a result fails\n",
      " |      priority : Number\n",
      " |          Optional prioritization of task.  Zero is default.\n",
      " |          Higher priorities take precedence\n",
      " |      fifo_timeout : timedelta str (defaults to '60s')\n",
      " |          Allowed amount of time between calls to consider the same priority\n",
      " |      resources : dict (defaults to {})\n",
      " |          Defines the `resources` each instance of this mapped task requires\n",
      " |          on the worker; e.g. ``{'GPU': 2}``.\n",
      " |          See :doc:`worker resources <resources>` for details on defining\n",
      " |          resources.\n",
      " |      actors : bool or dict (default None)\n",
      " |          Whether these tasks should exist on the worker as stateful actors.\n",
      " |          Specified on a global (True/False) or per-task (``{'x': True,\n",
      " |          'y': False}``) basis. See :doc:`actors` for additional details.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List of collections, or single collection, depending on type of input.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> xx = client.persist(x)  # doctest: +SKIP\n",
      " |      >>> xx, yy = client.persist([x, y])  # doctest: +SKIP\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.compute\n",
      " |  \n",
      " |  processing(self, workers=None)\n",
      " |      The tasks currently running on each worker\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      workers : list (optional)\n",
      " |          A list of worker addresses, defaults to all\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x, y, z = c.map(inc, [1, 2, 3])  # doctest: +SKIP\n",
      " |      >>> c.processing()  # doctest: +SKIP\n",
      " |      {'192.168.1.141:46784': ['inc-1c8dd6be1c21646c71f76c16d09304ea',\n",
      " |                               'inc-fd65c238a7ea60f6a01bf4c8a5fcf44b',\n",
      " |                               'inc-1e297fc27658d7b67b3a758f16bcf47a']}\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.who_has\n",
      " |      Client.has_what\n",
      " |      Client.nthreads\n",
      " |  \n",
      " |  profile(self, key=None, start=None, stop=None, workers=None, merge_workers=True, plot=False, filename=None, server=False, scheduler=False)\n",
      " |      Collect statistical profiling information about recent work\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          Key prefix to select, this is typically a function name like 'inc'\n",
      " |          Leave as None to collect all data\n",
      " |      start : time\n",
      " |      stop : time\n",
      " |      workers : list\n",
      " |          List of workers to restrict profile information\n",
      " |      server : bool\n",
      " |          If true, return the profile of the worker's administrative thread\n",
      " |          rather than the worker threads.\n",
      " |          This is useful when profiling Dask itself, rather than user code.\n",
      " |      scheduler : bool\n",
      " |          If true, return the profile information from the scheduler's\n",
      " |          administrative thread rather than the workers.\n",
      " |          This is useful when profiling Dask's scheduling itself.\n",
      " |      plot : boolean or string\n",
      " |          Whether or not to return a plot object\n",
      " |      filename : str\n",
      " |          Filename to save the plot\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> client.profile()  # call on collections\n",
      " |      >>> client.profile(filename='dask-profile.html')  # save to html file\n",
      " |  \n",
      " |  publish_dataset(self, *args, **kwargs)\n",
      " |      Publish named datasets to scheduler\n",
      " |      \n",
      " |      This stores a named reference to a dask collection or list of futures\n",
      " |      on the scheduler.  These references are available to other Clients\n",
      " |      which can download the collection or futures with ``get_dataset``.\n",
      " |      \n",
      " |      Datasets are not immediately computed.  You may wish to call\n",
      " |      ``Client.persist`` prior to publishing a dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      args : list of objects to publish as name\n",
      " |      kwargs : dict\n",
      " |          named collections to publish on the scheduler\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Publishing client:\n",
      " |      \n",
      " |      >>> df = dd.read_csv('s3://...')  # doctest: +SKIP\n",
      " |      >>> df = c.persist(df) # doctest: +SKIP\n",
      " |      >>> c.publish_dataset(my_dataset=df)  # doctest: +SKIP\n",
      " |      \n",
      " |      Alternative invocation\n",
      " |      >>> c.publish_dataset(df, name='my_dataset')\n",
      " |      \n",
      " |      Receiving client:\n",
      " |      \n",
      " |      >>> c.list_datasets()  # doctest: +SKIP\n",
      " |      ['my_dataset']\n",
      " |      >>> df2 = c.get_dataset('my_dataset')  # doctest: +SKIP\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.list_datasets\n",
      " |      Client.get_dataset\n",
      " |      Client.unpublish_dataset\n",
      " |      Client.persist\n",
      " |  \n",
      " |  rebalance(self, futures=None, workers=None, **kwargs)\n",
      " |      Rebalance data within network\n",
      " |      \n",
      " |      Move data between workers to roughly balance memory burden.  This\n",
      " |      either affects a subset of the keys/workers or the entire network,\n",
      " |      depending on keyword arguments.\n",
      " |      \n",
      " |      For details on the algorithm and configuration options, refer to the matching\n",
      " |      scheduler-side method :meth:`~distributed.scheduler.Scheduler.rebalance`.\n",
      " |      \n",
      " |      .. warning::\n",
      " |         This operation is generally not well tested against normal operation of the\n",
      " |         scheduler. It is not recommended to use it while waiting on computations.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      futures : list, optional\n",
      " |          A list of futures to balance, defaults all data\n",
      " |      workers : list, optional\n",
      " |          A list of workers on which to balance, defaults to all workers\n",
      " |      **kwargs : dict\n",
      " |          Optional keyword arguments for the function\n",
      " |  \n",
      " |  register_plugin(self, plugin: 'NannyPlugin | SchedulerPlugin | WorkerPlugin', name: 'str | None' = None, idempotent: 'bool | None' = None)\n",
      " |      Register a plugin.\n",
      " |      \n",
      " |      See https://distributed.readthedocs.io/en/latest/plugins.html\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      plugin :\n",
      " |          A nanny, scheduler, or worker plugin to register.\n",
      " |      name :\n",
      " |          Name for the plugin; if None, a name is taken from the\n",
      " |          plugin instance or automatically generated if not present.\n",
      " |      idempotent :\n",
      " |          Do not re-register if a plugin of the given name already exists.\n",
      " |          If None, ``plugin.idempotent`` is taken if defined, False otherwise.\n",
      " |  \n",
      " |  register_scheduler_plugin(self, plugin: 'SchedulerPlugin', name: 'str | None' = None, idempotent: 'bool | None' = None)\n",
      " |      Register a scheduler plugin.\n",
      " |      \n",
      " |      .. deprecated:: 2023.9.2\n",
      " |          Use :meth:`Client.register_plugin` instead.\n",
      " |      \n",
      " |      See https://distributed.readthedocs.io/en/latest/plugins.html#scheduler-plugins\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      plugin : SchedulerPlugin\n",
      " |          SchedulerPlugin instance to pass to the scheduler.\n",
      " |      name : str\n",
      " |          Name for the plugin; if None, a name is taken from the\n",
      " |          plugin instance or automatically generated if not present.\n",
      " |      idempotent : bool\n",
      " |          Do not re-register if a plugin of the given name already exists.\n",
      " |  \n",
      " |  register_worker_callbacks(self, setup=None)\n",
      " |      Registers a setup callback function for all current and future workers.\n",
      " |      \n",
      " |      This registers a new setup function for workers in this cluster. The\n",
      " |      function will run immediately on all currently connected workers. It\n",
      " |      will also be run upon connection by any workers that are added in the\n",
      " |      future. Multiple setup functions can be registered - these will be\n",
      " |      called in the order they were added.\n",
      " |      \n",
      " |      If the function takes an input argument named ``dask_worker`` then\n",
      " |      that variable will be populated with the worker itself.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      setup : callable(dask_worker: Worker) -> None\n",
      " |          Function to register and run on all workers\n",
      " |  \n",
      " |  register_worker_plugin(self, plugin: 'NannyPlugin | WorkerPlugin', name: 'str | None' = None, nanny: 'bool | None' = None)\n",
      " |      Registers a lifecycle worker plugin for all current and future workers.\n",
      " |      \n",
      " |      .. deprecated:: 2023.9.2\n",
      " |          Use :meth:`Client.register_plugin` instead.\n",
      " |      \n",
      " |      This registers a new object to handle setup, task state transitions and\n",
      " |      teardown for workers in this cluster. The plugin will instantiate\n",
      " |      itself on all currently connected workers. It will also be run on any\n",
      " |      worker that connects in the future.\n",
      " |      \n",
      " |      The plugin may include methods ``setup``, ``teardown``, ``transition``,\n",
      " |      and ``release_key``.  See the\n",
      " |      ``dask.distributed.WorkerPlugin`` class or the examples below for the\n",
      " |      interface and docstrings.  It must be serializable with the pickle or\n",
      " |      cloudpickle modules.\n",
      " |      \n",
      " |      If the plugin has a ``name`` attribute, or if the ``name=`` keyword is\n",
      " |      used then that will control idempotency.  If a plugin with that name has\n",
      " |      already been registered, then it will be removed and replaced by the new one.\n",
      " |      \n",
      " |      For alternatives to plugins, you may also wish to look into preload\n",
      " |      scripts.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      plugin : WorkerPlugin or NannyPlugin\n",
      " |          WorkerPlugin or NannyPlugin instance to register.\n",
      " |      name : str, optional\n",
      " |          A name for the plugin.\n",
      " |          Registering a plugin with the same name will have no effect.\n",
      " |          If plugin has no name attribute a random name is used.\n",
      " |      nanny : bool, optional\n",
      " |          Whether to register the plugin with workers or nannies.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> class MyPlugin(WorkerPlugin):\n",
      " |      ...     def __init__(self, *args, **kwargs):\n",
      " |      ...         pass  # the constructor is up to you\n",
      " |      ...     def setup(self, worker: dask.distributed.Worker):\n",
      " |      ...         pass\n",
      " |      ...     def teardown(self, worker: dask.distributed.Worker):\n",
      " |      ...         pass\n",
      " |      ...     def transition(self, key: str, start: str, finish: str,\n",
      " |      ...                    **kwargs):\n",
      " |      ...         pass\n",
      " |      ...     def release_key(self, key: str, state: str, cause: str | None, reason: None, report: bool):\n",
      " |      ...         pass\n",
      " |      \n",
      " |      >>> plugin = MyPlugin(1, 2, 3)\n",
      " |      >>> client.register_plugin(plugin)\n",
      " |      \n",
      " |      You can get access to the plugin with the ``get_worker`` function\n",
      " |      \n",
      " |      >>> client.register_plugin(other_plugin, name='my-plugin')\n",
      " |      >>> def f():\n",
      " |      ...    worker = get_worker()\n",
      " |      ...    plugin = worker.plugins['my-plugin']\n",
      " |      ...    return plugin.my_state\n",
      " |      \n",
      " |      >>> future = client.run(f)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      distributed.WorkerPlugin\n",
      " |      unregister_worker_plugin\n",
      " |  \n",
      " |  replicate(self, futures, n=None, workers=None, branching_factor=2, **kwargs)\n",
      " |      Set replication of futures within network\n",
      " |      \n",
      " |      Copy data onto many workers.  This helps to broadcast frequently\n",
      " |      accessed data and can improve resilience.\n",
      " |      \n",
      " |      This performs a tree copy of the data throughout the network\n",
      " |      individually on each piece of data.  This operation blocks until\n",
      " |      complete.  It does not guarantee replication of data to future workers.\n",
      " |      \n",
      " |      .. note::\n",
      " |         This method is incompatible with the Active Memory Manager's\n",
      " |         :ref:`ReduceReplicas` policy. If you wish to use it, you must first disable\n",
      " |         the policy or disable the AMM entirely.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      futures : list of futures\n",
      " |          Futures we wish to replicate\n",
      " |      n : int, optional\n",
      " |          Number of processes on the cluster on which to replicate the data.\n",
      " |          Defaults to all.\n",
      " |      workers : list of worker addresses\n",
      " |          Workers on which we want to restrict the replication.\n",
      " |          Defaults to all.\n",
      " |      branching_factor : int, optional\n",
      " |          The number of workers that can copy data in each generation\n",
      " |      **kwargs : dict\n",
      " |          Optional keyword arguments for the remote function\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = c.submit(func, *args)  # doctest: +SKIP\n",
      " |      >>> c.replicate([x])  # send to all workers  # doctest: +SKIP\n",
      " |      >>> c.replicate([x], n=3)  # send to three workers  # doctest: +SKIP\n",
      " |      >>> c.replicate([x], workers=['alice', 'bob'])  # send to specific  # doctest: +SKIP\n",
      " |      >>> c.replicate([x], n=1, workers=['alice', 'bob'])  # send to one of specific workers  # doctest: +SKIP\n",
      " |      >>> c.replicate([x], n=1)  # reduce replications # doctest: +SKIP\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.rebalance\n",
      " |  \n",
      " |  restart(self, timeout: 'str | int | float | NoDefault' = <no_default>, wait_for_workers: 'bool' = True)\n",
      " |      Restart all workers. Reset local state. Optionally wait for workers to return.\n",
      " |      \n",
      " |      Workers without nannies are shut down, hoping an external deployment system\n",
      " |      will restart them. Therefore, if not using nannies and your deployment system\n",
      " |      does not automatically restart workers, ``restart`` will just shut down all\n",
      " |      workers, then time out!\n",
      " |      \n",
      " |      After ``restart``, all connected workers are new, regardless of whether ``TimeoutError``\n",
      " |      was raised. Any workers that failed to shut down in time are removed, and\n",
      " |      may or may not shut down on their own in the future.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      timeout:\n",
      " |          How long to wait for workers to shut down and come back, if ``wait_for_workers``\n",
      " |          is True, otherwise just how long to wait for workers to shut down.\n",
      " |          Raises ``asyncio.TimeoutError`` if this is exceeded.\n",
      " |      wait_for_workers:\n",
      " |          Whether to wait for all workers to reconnect, or just for them to shut down\n",
      " |          (default True). Use ``restart(wait_for_workers=False)`` combined with\n",
      " |          :meth:`Client.wait_for_workers` for granular control over how many workers to\n",
      " |          wait for.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      Scheduler.restart\n",
      " |      Client.restart_workers\n",
      " |  \n",
      " |  restart_workers(self, workers: 'list[str]', timeout: 'str | int | float | NoDefault' = <no_default>, raise_for_error: 'bool' = True)\n",
      " |      Restart a specified set of workers\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          Only workers being monitored by a :class:`distributed.Nanny` can be restarted.\n",
      " |          See ``Nanny.restart`` for more details.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      workers : list[str]\n",
      " |          Workers to restart. This can be a list of worker addresses, names, or a both.\n",
      " |      timeout : int | float | None\n",
      " |          Number of seconds to wait\n",
      " |      raise_for_error: bool (default True)\n",
      " |          Whether to raise a :py:class:`TimeoutError` if restarting worker(s) doesn't\n",
      " |          finish within ``timeout``, or another exception caused from restarting\n",
      " |          worker(s).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict[str, \"OK\" | \"removed\" | \"timed out\"]\n",
      " |          Mapping of worker and restart status, the keys will match the original\n",
      " |          values passed in via ``workers``.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method differs from :meth:`Client.restart` in that this method\n",
      " |      simply restarts the specified set of workers, while ``Client.restart``\n",
      " |      will restart all workers and also reset local state on the cluster\n",
      " |      (e.g. all keys are released).\n",
      " |      \n",
      " |      Additionally, this method does not gracefully handle tasks that are\n",
      " |      being executed when a worker is restarted. These tasks may fail or have\n",
      " |      their suspicious count incremented.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      You can get information about active workers using the following:\n",
      " |      \n",
      " |      >>> workers = client.scheduler_info()['workers']\n",
      " |      \n",
      " |      From that list you may want to select some workers to restart\n",
      " |      \n",
      " |      >>> client.restart_workers(workers=['tcp://address:port', ...])\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.restart\n",
      " |  \n",
      " |  retire_workers(self, workers: 'list[str] | None' = None, close_workers: 'bool' = True, **kwargs)\n",
      " |      Retire certain workers on the scheduler\n",
      " |      \n",
      " |      See :meth:`distributed.Scheduler.retire_workers` for the full docstring.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      workers\n",
      " |      close_workers\n",
      " |      **kwargs : dict\n",
      " |          Optional keyword arguments for the remote function\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      You can get information about active workers using the following:\n",
      " |      \n",
      " |      >>> workers = client.scheduler_info()['workers']\n",
      " |      \n",
      " |      From that list you may want to select some workers to close\n",
      " |      \n",
      " |      >>> client.retire_workers(workers=['tcp://address:port', ...])\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.distributed.Scheduler.retire_workers\n",
      " |  \n",
      " |  retry(self, futures, asynchronous=None)\n",
      " |      Retry failed futures\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      futures : list of Futures\n",
      " |          The list of Futures\n",
      " |      asynchronous: bool\n",
      " |          If True the client is in asynchronous mode\n",
      " |  \n",
      " |  run(self, function, *args, workers: 'list[str] | None' = None, wait: 'bool' = True, nanny: 'bool' = False, on_error: \"Literal['raise', 'return', 'ignore']\" = 'raise', **kwargs)\n",
      " |      Run a function on all workers outside of task scheduling system\n",
      " |      \n",
      " |      This calls a function on all currently known workers immediately,\n",
      " |      blocks until those results come back, and returns the results\n",
      " |      asynchronously as a dictionary keyed by worker address.  This method\n",
      " |      is generally used for side effects such as collecting diagnostic\n",
      " |      information or installing libraries.\n",
      " |      \n",
      " |      If your function takes an input argument named ``dask_worker`` then\n",
      " |      that variable will be populated with the worker itself.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      function : callable\n",
      " |          The function to run\n",
      " |      *args : tuple\n",
      " |          Optional arguments for the remote function\n",
      " |      **kwargs : dict\n",
      " |          Optional keyword arguments for the remote function\n",
      " |      workers : list\n",
      " |          Workers on which to run the function. Defaults to all known\n",
      " |          workers.\n",
      " |      wait : boolean (optional)\n",
      " |          If the function is asynchronous whether or not to wait until that\n",
      " |          function finishes.\n",
      " |      nanny : bool, default False\n",
      " |          Whether to run ``function`` on the nanny. By default, the function\n",
      " |          is run on the worker process.  If specified, the addresses in\n",
      " |          ``workers`` should still be the worker addresses, not the nanny addresses.\n",
      " |      on_error: \"raise\" | \"return\" | \"ignore\"\n",
      " |          If the function raises an error on a worker:\n",
      " |      \n",
      " |          raise\n",
      " |              (default) Re-raise the exception on the client.\n",
      " |              The output from other workers will be lost.\n",
      " |          return\n",
      " |              Return the Exception object instead of the function output for\n",
      " |              the worker\n",
      " |          ignore\n",
      " |              Ignore the exception and remove the worker from the result dict\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> c.run(os.getpid)  # doctest: +SKIP\n",
      " |      {'192.168.0.100:9000': 1234,\n",
      " |       '192.168.0.101:9000': 4321,\n",
      " |       '192.168.0.102:9000': 5555}\n",
      " |      \n",
      " |      Restrict computation to particular workers with the ``workers=``\n",
      " |      keyword argument.\n",
      " |      \n",
      " |      >>> c.run(os.getpid, workers=['192.168.0.100:9000',\n",
      " |      ...                           '192.168.0.101:9000'])  # doctest: +SKIP\n",
      " |      {'192.168.0.100:9000': 1234,\n",
      " |       '192.168.0.101:9000': 4321}\n",
      " |      \n",
      " |      >>> def get_status(dask_worker):\n",
      " |      ...     return dask_worker.status\n",
      " |      \n",
      " |      >>> c.run(get_status)  # doctest: +SKIP\n",
      " |      {'192.168.0.100:9000': 'running',\n",
      " |       '192.168.0.101:9000': 'running}\n",
      " |      \n",
      " |      Run asynchronous functions in the background:\n",
      " |      \n",
      " |      >>> async def print_state(dask_worker):  # doctest: +SKIP\n",
      " |      ...    while True:\n",
      " |      ...        print(dask_worker.status)\n",
      " |      ...        await asyncio.sleep(1)\n",
      " |      \n",
      " |      >>> c.run(print_state, wait=False)  # doctest: +SKIP\n",
      " |  \n",
      " |  run_on_scheduler(self, function, *args, **kwargs)\n",
      " |      Run a function on the scheduler process\n",
      " |      \n",
      " |      This is typically used for live debugging.  The function should take a\n",
      " |      keyword argument ``dask_scheduler=``, which will be given the scheduler\n",
      " |      object itself.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      function : callable\n",
      " |          The function to run on the scheduler process\n",
      " |      *args : tuple\n",
      " |          Optional arguments for the function\n",
      " |      **kwargs : dict\n",
      " |          Optional keyword arguments for the function\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def get_number_of_tasks(dask_scheduler=None):\n",
      " |      ...     return len(dask_scheduler.tasks)\n",
      " |      \n",
      " |      >>> client.run_on_scheduler(get_number_of_tasks)  # doctest: +SKIP\n",
      " |      100\n",
      " |      \n",
      " |      Run asynchronous functions in the background:\n",
      " |      \n",
      " |      >>> async def print_state(dask_scheduler):  # doctest: +SKIP\n",
      " |      ...    while True:\n",
      " |      ...        print(dask_scheduler.status)\n",
      " |      ...        await asyncio.sleep(1)\n",
      " |      \n",
      " |      >>> c.run(print_state, wait=False)  # doctest: +SKIP\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.run : Run a function on all workers\n",
      " |  \n",
      " |  scatter(self, data, workers=None, broadcast=False, direct=None, hash=True, timeout=<no_default>, asynchronous=None)\n",
      " |      Scatter data into distributed memory\n",
      " |      \n",
      " |      This moves data from the local client process into the workers of the\n",
      " |      distributed scheduler.  Note that it is often better to submit jobs to\n",
      " |      your workers to have them load the data rather than loading data\n",
      " |      locally and then scattering it out to them.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : list, dict, or object\n",
      " |          Data to scatter out to workers.  Output type matches input type.\n",
      " |      workers : list of tuples (optional)\n",
      " |          Optionally constrain locations of data.\n",
      " |          Specify workers as hostname/port pairs, e.g.\n",
      " |          ``('127.0.0.1', 8787)``.\n",
      " |      broadcast : bool (defaults to False)\n",
      " |          Whether to send each data element to all workers.\n",
      " |          By default we round-robin based on number of cores.\n",
      " |      \n",
      " |          .. note::\n",
      " |             Setting this flag to True is incompatible with the Active Memory\n",
      " |             Manager's :ref:`ReduceReplicas` policy. If you wish to use it, you must\n",
      " |             first disable the policy or disable the AMM entirely.\n",
      " |      direct : bool (defaults to automatically check)\n",
      " |          Whether or not to connect directly to the workers, or to ask\n",
      " |          the scheduler to serve as intermediary.  This can also be set when\n",
      " |          creating the Client.\n",
      " |      hash : bool (optional)\n",
      " |          Whether or not to hash data to determine key.\n",
      " |          If False then this uses a random key\n",
      " |      timeout : number, optional\n",
      " |          Time in seconds after which to raise a\n",
      " |          ``dask.distributed.TimeoutError``\n",
      " |      asynchronous: bool\n",
      " |          If True the client is in asynchronous mode\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List, dict, iterator, or queue of futures matching the type of input.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> c = Client('127.0.0.1:8787')  # doctest: +SKIP\n",
      " |      >>> c.scatter(1) # doctest: +SKIP\n",
      " |      <Future: status: finished, key: c0a8a20f903a4915b94db8de3ea63195>\n",
      " |      \n",
      " |      >>> c.scatter([1, 2, 3])  # doctest: +SKIP\n",
      " |      [<Future: status: finished, key: c0a8a20f903a4915b94db8de3ea63195>,\n",
      " |       <Future: status: finished, key: 58e78e1b34eb49a68c65b54815d1b158>,\n",
      " |       <Future: status: finished, key: d3395e15f605bc35ab1bac6341a285e2>]\n",
      " |      \n",
      " |      >>> c.scatter({'x': 1, 'y': 2, 'z': 3})  # doctest: +SKIP\n",
      " |      {'x': <Future: status: finished, key: x>,\n",
      " |       'y': <Future: status: finished, key: y>,\n",
      " |       'z': <Future: status: finished, key: z>}\n",
      " |      \n",
      " |      Constrain location of data to subset of workers\n",
      " |      \n",
      " |      >>> c.scatter([1, 2, 3], workers=[('hostname', 8788)])   # doctest: +SKIP\n",
      " |      \n",
      " |      Broadcast data to all workers\n",
      " |      \n",
      " |      >>> [future] = c.scatter([element], broadcast=True)  # doctest: +SKIP\n",
      " |      \n",
      " |      Send scattered data to parallelized function using client futures\n",
      " |      interface\n",
      " |      \n",
      " |      >>> data = c.scatter(data, broadcast=True)  # doctest: +SKIP\n",
      " |      >>> res = [c.submit(func, data, i) for i in range(100)]\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Scattering a dictionary uses ``dict`` keys to create ``Future`` keys.\n",
      " |      The current implementation of a task graph resolution searches for occurrences of ``key``\n",
      " |      and replaces it with a corresponding ``Future`` result. That can lead to unwanted\n",
      " |      substitution of strings passed as arguments to a task if these strings match some ``key``\n",
      " |      that already exists on a cluster. To avoid these situations it is required to use unique\n",
      " |      values if a ``key`` is set manually.\n",
      " |      See https://github.com/dask/dask/issues/9969 to track progress on resolving this issue.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.gather : Gather data back to local process\n",
      " |  \n",
      " |  scheduler_info(self, n_workers: 'int' = 5, **kwargs: 'Any') -> 'SchedulerInfo'\n",
      " |      Basic information about the workers in the cluster\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n_workers: int\n",
      " |          The number of workers for which to fetch information. To fetch all,\n",
      " |          use -1.\n",
      " |      **kwargs : dict\n",
      " |          Optional keyword arguments for the remote function\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> c.scheduler_info()  # doctest: +SKIP\n",
      " |      {'id': '2de2b6da-69ee-11e6-ab6a-e82aea155996',\n",
      " |       'services': {},\n",
      " |       'type': 'Scheduler',\n",
      " |       'workers': {'127.0.0.1:40575': {'active': 0,\n",
      " |                                       'last-seen': 1472038237.4845693,\n",
      " |                                       'name': '127.0.0.1:40575',\n",
      " |                                       'services': {},\n",
      " |                                       'stored': 0,\n",
      " |                                       'time-delay': 0.0061032772064208984}}}\n",
      " |  \n",
      " |  set_metadata(self, key, value)\n",
      " |      Set arbitrary metadata in the scheduler\n",
      " |      \n",
      " |      This allows you to store small amounts of data on the central scheduler\n",
      " |      process for administrative purposes.  Data should be msgpack\n",
      " |      serializable (ints, strings, lists, dicts)\n",
      " |      \n",
      " |      If the key corresponds to a task then that key will be cleaned up when\n",
      " |      the task is forgotten by the scheduler.\n",
      " |      \n",
      " |      If the key is a list then it will be assumed that you want to index\n",
      " |      into a nested dictionary structure using those keys.  For example if\n",
      " |      you call the following::\n",
      " |      \n",
      " |          >>> client.set_metadata(['a', 'b', 'c'], 123)\n",
      " |      \n",
      " |      Then this is the same as setting\n",
      " |      \n",
      " |          >>> scheduler.task_metadata['a']['b']['c'] = 123\n",
      " |      \n",
      " |      The lower level dictionaries will be created on demand.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> client.set_metadata('x', 123)  # doctest: +SKIP\n",
      " |      >>> client.get_metadata('x')  # doctest: +SKIP\n",
      " |      123\n",
      " |      \n",
      " |      >>> client.set_metadata(['x', 'y'], 123)  # doctest: +SKIP\n",
      " |      >>> client.get_metadata('x')  # doctest: +SKIP\n",
      " |      {'y': 123}\n",
      " |      \n",
      " |      >>> client.set_metadata(['x', 'w', 'z'], 456)  # doctest: +SKIP\n",
      " |      >>> client.get_metadata('x')  # doctest: +SKIP\n",
      " |      {'y': 123, 'w': {'z': 456}}\n",
      " |      \n",
      " |      >>> client.get_metadata(['x', 'w'])  # doctest: +SKIP\n",
      " |      {'z': 456}\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      get_metadata\n",
      " |  \n",
      " |  shutdown(self)\n",
      " |      Shut down the connected scheduler and workers\n",
      " |      \n",
      " |      Note, this may disrupt other clients that may be using the same\n",
      " |      scheduler and workers.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.close : close only this client\n",
      " |  \n",
      " |  start(self, **kwargs)\n",
      " |      Start scheduler running in separate thread\n",
      " |  \n",
      " |  story(self, *keys_or_stimuli, on_error='raise')\n",
      " |      Returns a cluster-wide story for the given keys or stimulus_id's\n",
      " |  \n",
      " |  submit(self, func, *args, key=None, workers=None, resources=None, retries=None, priority=0, fifo_timeout='100 ms', allow_other_workers=False, actor=False, actors=False, pure=True, **kwargs)\n",
      " |      Submit a function application to the scheduler\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : callable\n",
      " |          Callable to be scheduled as ``func(*args **kwargs)``. If ``func`` returns a\n",
      " |          coroutine, it will be run on the main event loop of a worker. Otherwise\n",
      " |          ``func`` will be run in a worker's task executor pool (see\n",
      " |          ``Worker.executors`` for more information.)\n",
      " |      *args : tuple\n",
      " |          Optional positional arguments\n",
      " |      key : str\n",
      " |          Unique identifier for the task.  Defaults to function-name and hash\n",
      " |      workers : string or iterable of strings\n",
      " |          A set of worker addresses or hostnames on which computations may be\n",
      " |          performed. Leave empty to default to all workers (common case)\n",
      " |      resources : dict (defaults to {})\n",
      " |          Defines the ``resources`` each instance of this mapped task\n",
      " |          requires on the worker; e.g. ``{'GPU': 2}``.\n",
      " |          See :doc:`worker resources <resources>` for details on defining\n",
      " |          resources.\n",
      " |      retries : int (default to 0)\n",
      " |          Number of allowed automatic retries if the task fails\n",
      " |      priority : Number\n",
      " |          Optional prioritization of task.  Zero is default.\n",
      " |          Higher priorities take precedence\n",
      " |      fifo_timeout : str timedelta (default '100ms')\n",
      " |          Allowed amount of time between calls to consider the same priority\n",
      " |      allow_other_workers : bool (defaults to False)\n",
      " |          Used with ``workers``. Indicates whether or not the computations\n",
      " |          may be performed on workers that are not in the `workers` set(s).\n",
      " |      actor : bool (default False)\n",
      " |          Whether this task should exist on the worker as a stateful actor.\n",
      " |          See :doc:`actors` for additional details.\n",
      " |      actors : bool (default False)\n",
      " |          Alias for `actor`\n",
      " |      pure : bool (defaults to True)\n",
      " |          Whether or not the function is pure.  Set ``pure=False`` for\n",
      " |          impure functions like ``np.random.random``. Note that if both\n",
      " |          ``actor`` and ``pure`` kwargs are set to True, then the value\n",
      " |          of ``pure`` will be reverted to False, since an actor is stateful.\n",
      " |          See :ref:`pure functions` for more details.\n",
      " |      **kwargs\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> c = client.submit(add, a, b)  # doctest: +SKIP\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The current implementation of a task graph resolution searches for occurrences of ``key``\n",
      " |      and replaces it with a corresponding ``Future`` result. That can lead to unwanted\n",
      " |      substitution of strings passed as arguments to a task if these strings match some ``key``\n",
      " |      that already exists on a cluster. To avoid these situations it is required to use unique\n",
      " |      values if a ``key`` is set manually.\n",
      " |      See https://github.com/dask/dask/issues/9969 to track progress on resolving this issue.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Future\n",
      " |          If running in asynchronous mode, returns the future. Otherwise\n",
      " |          returns the concrete value\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      TypeError\n",
      " |          If 'func' is not callable, a TypeError is raised\n",
      " |      ValueError\n",
      " |          If 'allow_other_workers'is True and 'workers' is None, a\n",
      " |          ValueError is raised\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.map : Submit on many arguments at once\n",
      " |  \n",
      " |  subscribe_topic(self, topic, handler)\n",
      " |      Subscribe to a topic and execute a handler for every received event\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topic: str\n",
      " |          The topic name\n",
      " |      handler: callable or coroutine function\n",
      " |          A handler called for every received event. The handler must accept a\n",
      " |          single argument `event` which is a tuple `(timestamp, msg)` where\n",
      " |          timestamp refers to the clock on the scheduler.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> import logging\n",
      " |      >>> logger = logging.getLogger(\"myLogger\")  # Log config not shown\n",
      " |      >>> client.subscribe_topic(\"topic-name\", lambda: logger.info)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.distributed.Client.unsubscribe_topic\n",
      " |      dask.distributed.Client.get_events\n",
      " |      dask.distributed.Client.log_event\n",
      " |  \n",
      " |  unforward_logging(self, logger_name=None)\n",
      " |      Stop forwarding the given logger (default root) from worker tasks to the\n",
      " |      client process.\n",
      " |  \n",
      " |  unpublish_dataset(self, name, **kwargs)\n",
      " |      Remove named datasets from scheduler\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          The name of the dataset to unpublish\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> c.list_datasets()  # doctest: +SKIP\n",
      " |      ['my_dataset']\n",
      " |      >>> c.unpublish_dataset('my_dataset')  # doctest: +SKIP\n",
      " |      >>> c.list_datasets()  # doctest: +SKIP\n",
      " |      []\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.publish_dataset\n",
      " |  \n",
      " |  unregister_scheduler_plugin(self, name)\n",
      " |      Unregisters a scheduler plugin\n",
      " |      \n",
      " |      See https://distributed.readthedocs.io/en/latest/plugins.html#scheduler-plugins\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Name of the plugin to unregister. See the :meth:`Client.register_scheduler_plugin`\n",
      " |          docstring for more information.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> class MyPlugin(SchedulerPlugin):\n",
      " |      ...     def __init__(self, *args, **kwargs):\n",
      " |      ...         pass  # the constructor is up to you\n",
      " |      ...     async def start(self, scheduler: Scheduler) -> None:\n",
      " |      ...         pass\n",
      " |      ...     async def before_close(self) -> None:\n",
      " |      ...         pass\n",
      " |      ...     async def close(self) -> None:\n",
      " |      ...         pass\n",
      " |      ...     def restart(self, scheduler: Scheduler) -> None:\n",
      " |      ...         pass\n",
      " |      \n",
      " |      >>> plugin = MyPlugin(1, 2, 3)\n",
      " |      >>> client.register_plugin(plugin, name='foo')\n",
      " |      >>> client.unregister_scheduler_plugin(name='foo')\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      register_scheduler_plugin\n",
      " |  \n",
      " |  unregister_worker_plugin(self, name, nanny=None)\n",
      " |      Unregisters a lifecycle worker plugin\n",
      " |      \n",
      " |      This unregisters an existing worker plugin. As part of the unregistration process\n",
      " |      the plugin's ``teardown`` method will be called.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Name of the plugin to unregister. See the :meth:`Client.register_plugin`\n",
      " |          docstring for more information.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> class MyPlugin(WorkerPlugin):\n",
      " |      ...     def __init__(self, *args, **kwargs):\n",
      " |      ...         pass  # the constructor is up to you\n",
      " |      ...     def setup(self, worker: dask.distributed.Worker):\n",
      " |      ...         pass\n",
      " |      ...     def teardown(self, worker: dask.distributed.Worker):\n",
      " |      ...         pass\n",
      " |      ...     def transition(self, key: str, start: str, finish: str, **kwargs):\n",
      " |      ...         pass\n",
      " |      ...     def release_key(self, key: str, state: str, cause: str | None, reason: None, report: bool):\n",
      " |      ...         pass\n",
      " |      \n",
      " |      >>> plugin = MyPlugin(1, 2, 3)\n",
      " |      >>> client.register_plugin(plugin, name='foo')\n",
      " |      >>> client.unregister_worker_plugin(name='foo')\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      register_plugin\n",
      " |  \n",
      " |  unsubscribe_topic(self, topic)\n",
      " |      Unsubscribe from a topic and remove event handler\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.distributed.Client.subscribe_topic\n",
      " |      dask.distributed.Client.get_events\n",
      " |      dask.distributed.Client.log_event\n",
      " |  \n",
      " |  upload_file(self, filename, load: 'bool' = True)\n",
      " |      Upload local package to scheduler and workers\n",
      " |      \n",
      " |      This sends a local file up to the scheduler and all worker nodes.\n",
      " |      This file is placed into the working directory of each node, see config option\n",
      " |      ``temporary-directory`` (defaults to :py:func:`tempfile.gettempdir`).\n",
      " |      \n",
      " |      This directory will be added to the Python's system path so any ``.py``,\n",
      " |      ``.egg`` or ``.zip``  files will be importable.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      filename : string\n",
      " |          Filename of ``.py``, ``.egg``, or ``.zip`` file to send to workers\n",
      " |      load : bool, optional\n",
      " |          Whether or not to import the module as part of the upload process.\n",
      " |          Defaults to ``True``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> client.upload_file('mylibrary.egg')  # doctest: +SKIP\n",
      " |      >>> from mylibrary import myfunc  # doctest: +SKIP\n",
      " |      >>> L = client.map(myfunc, seq)  # doctest: +SKIP\n",
      " |      >>>\n",
      " |      >>> # Where did that file go? Use `dask_worker.local_directory`.\n",
      " |      >>> def where_is_mylibrary(dask_worker):\n",
      " |      >>>     path = pathlib.Path(dask_worker.local_directory) / 'mylibrary.egg'\n",
      " |      >>>     assert path.exists()\n",
      " |      >>>     return str(path)\n",
      " |      >>>\n",
      " |      >>> client.run(where_is_mylibrary)  # doctest: +SKIP\n",
      " |  \n",
      " |  wait_for_workers(self, n_workers: 'int', timeout: 'float | None' = None) -> 'None'\n",
      " |      Blocking call to wait for n workers before continuing\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n_workers : int\n",
      " |          The number of workers\n",
      " |      timeout : number, optional\n",
      " |          Time in seconds after which to raise a\n",
      " |          ``dask.distributed.TimeoutError``\n",
      " |  \n",
      " |  who_has(self, futures=None, **kwargs)\n",
      " |      The workers storing each future's data\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      futures : list (optional)\n",
      " |          A list of futures, defaults to all data\n",
      " |      **kwargs : dict\n",
      " |          Optional keyword arguments for the remote function\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x, y, z = c.map(inc, [1, 2, 3])  # doctest: +SKIP\n",
      " |      >>> wait([x, y, z])  # doctest: +SKIP\n",
      " |      >>> c.who_has()  # doctest: +SKIP\n",
      " |      {'inc-1c8dd6be1c21646c71f76c16d09304ea': ['192.168.1.141:46784'],\n",
      " |       'inc-1e297fc27658d7b67b3a758f16bcf47a': ['192.168.1.141:46784'],\n",
      " |       'inc-fd65c238a7ea60f6a01bf4c8a5fcf44b': ['192.168.1.141:46784']}\n",
      " |      \n",
      " |      >>> c.who_has([x, y])  # doctest: +SKIP\n",
      " |      {'inc-1c8dd6be1c21646c71f76c16d09304ea': ['192.168.1.141:46784'],\n",
      " |       'inc-1e297fc27658d7b67b3a758f16bcf47a': ['192.168.1.141:46784']}\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.has_what\n",
      " |      Client.nthreads\n",
      " |  \n",
      " |  write_scheduler_file(self, scheduler_file)\n",
      " |      Write the scheduler information to a json file.\n",
      " |      \n",
      " |      This facilitates easy sharing of scheduler information using a file\n",
      " |      system. The scheduler file can be used to instantiate a second Client\n",
      " |      using the same scheduler.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      scheduler_file : str\n",
      " |          Path to a write the scheduler file.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> client = Client()  # doctest: +SKIP\n",
      " |      >>> client.write_scheduler_file('scheduler.json')  # doctest: +SKIP\n",
      " |      # connect to previous client's scheduler\n",
      " |      >>> client2 = Client(scheduler_file='scheduler.json')  # doctest: +SKIP\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  current(allow_global=True) from builtins.type\n",
      " |      When running within the context of `as_client`, return the context-local\n",
      " |      current client. Otherwise, return the latest initialised Client.\n",
      " |      If no Client instances exist, raise ValueError.\n",
      " |      If allow_global is set to False, raise ValueError if running outside of\n",
      " |      the `as_client` context manager.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      allow_global : bool\n",
      " |          If True returns the default client\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Client\n",
      " |          The current client\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          If there is no client set, a ValueError is raised\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      default_client\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  amm\n",
      " |      Convenience accessors for the :doc:`active_memory_manager`\n",
      " |  \n",
      " |  dashboard_link\n",
      " |      Link to the scheduler's dashboard.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Dashboard URL.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Opening the dashboard in your default web browser:\n",
      " |      \n",
      " |      >>> import webbrowser\n",
      " |      >>> from distributed import Client\n",
      " |      >>> client = Client()\n",
      " |      >>> webbrowser.open(client.dashboard_link)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  io_loop\n",
      " |  \n",
      " |  loop\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_Client__loop': 'IOLoop | None', '_instances': 'Cl...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from distributed.utils.SyncMethodMixin:\n",
      " |  \n",
      " |  sync(self, func, *args, asynchronous=None, callback_timeout=None, **kwargs)\n",
      " |      Call `func` with `args` synchronously or asynchronously depending on\n",
      " |      the calling context\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from distributed.utils.SyncMethodMixin:\n",
      " |  \n",
      " |  asynchronous\n",
      " |      Are we running in the event loop?\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from distributed.utils.SyncMethodMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model._dask_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MNIST Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(imgs_train[:1000, :], labels_train[:1000, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.multiprocessing as dm\n",
    "import dask.array as da\n",
    "def hello(arg):\n",
    "    return [1]\n",
    "with dm.multiprocessing.Pool(15) as p:\n",
    "    decoders = da.stack(\n",
    "        *[\n",
    "            p.starmap(\n",
    "                func=hello,\n",
    "                iterable= [\n",
    "                    (\n",
    "                        '1'\n",
    "                    )\n",
    "                    for idx_neuron in range(100)\n",
    "                ],\n",
    "            ),\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "decoders.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from nonlinear_approximator.inference import infer\n",
    "import h5py\n",
    "from nonlinear_approximator.model import NonlinearRegressorModel\n",
    "def predict(self, input_x: NDArray[np.floating], average: bool=True) -> NDArray[np.floating]:\n",
    "    \n",
    "    if not self.config.storage_path: # in memory mode \n",
    "        if self.decoders is None:\n",
    "            raise RuntimeError(\"The provided model has not been trained so cannot make a prediction. Call 'fit' first or 'load' first.\")\n",
    "        \n",
    "        outputs = infer(input_x, self.neurons, self.decoders, self.config)\n",
    "        \n",
    "        if average: \n",
    "            return outputs.mean(axis=2).T\n",
    "        else:\n",
    "            return outputs\n",
    "    else: # use persistent storage\n",
    "        self._check_storage_path_configured()\n",
    "        with h5py.File(self.config.storage_path.resolve().absolute(), 'a') as file:\n",
    "            input_dim, num_samples = input_x.shape\n",
    "            # use a recursive average to accumulate outputs from multiple chunks of neurons so we don't load all into memory at once \n",
    "            output_avg = None\n",
    "            count_avg = 0\n",
    "            \n",
    "            for neuron_chunk in file[NonlinearRegressorModel.DECODER_STRPATH].iter_chunks():\n",
    "                print(f\"neuron chunk: {neuron_chunk}; count_avg: {count_avg}\")            \n",
    "                # get neural activations for this batch of data                         \n",
    "                slice_start = neuron_chunk[2].start\n",
    "                slice_end = self.config.width if not hasattr(neuron_chunk[2], 'end') else neuron_chunk[2].end\n",
    "                \n",
    "                if np.any(np.isnan(\n",
    "                    file[NonlinearRegressorModel.DECODER_STRPATH][neuron_chunk]\n",
    "                )):\n",
    "                    raise ValueError(f\"Recorded Decoders for slice {neuron_chunk} contained NaN values: {file[NonlinearRegressorModel.DECODER_STRPATH][neuron_chunk]}\")\n",
    "                \n",
    "                output = infer(\n",
    "                    input_x, \n",
    "                    self.neurons[:, slice_start:slice_end], \n",
    "                    file[NonlinearRegressorModel.DECODER_STRPATH][neuron_chunk], \n",
    "                    self.config\n",
    "                ).mean(axis=2).T\n",
    "                \n",
    "                count = slice_end - slice_start + 1\n",
    "\n",
    "                if count_avg == 0:\n",
    "                    output_avg = output\n",
    "                    count_avg = count\n",
    "                \n",
    "                else:\n",
    "                    output_avg = (output * count + output_avg * count_avg) / (count + count_avg)\n",
    "                    count_avg += count \n",
    "                    \n",
    "            return output_avg.T\n",
    "model.predict = partial(predict, model)\n",
    "probs_train = model.predict(imgs_train[:,:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = probs_train.T.argmax(axis=1)\n",
    "print(preds_train.shape)\n",
    "acc_train = sum(preds_train==labels_train.argmax(axis=0)[:1000]).T / len(preds_train)\n",
    "\n",
    "plt.hist(preds_train, bins=10)\n",
    "plt.hist(labels_train.argmax(axis=0)[:1000], bins=10, alpha=0.3)\n",
    "plt.title(f\"Histogram of model predicted classifications on training data. Accuracy = {100 *acc_train}%\")\n",
    "plt.xlabel(\"Digit\")\n",
    "plt.ylabel(f\"Number of classifciations (N={len(preds_train)})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_test = model.predict(imgs_train[:,:1000]) \n",
    "preds_test = probs_test.T.argmax(axis=0)\n",
    "acc_test = sum(preds_test==labels_test.argmax(axis=0)[:1000]) / len(preds_test)\n",
    "\n",
    "plt.hist(preds_test, bins=10)\n",
    "plt.hist(labels_test.argmax(axis=0)[:1000], bins=10, alpha=0.3)\n",
    "plt.title(f\"Histogram of model predicted classifications on test data. Accuracy = {100 *acc_test}%\")\n",
    "plt.xlabel(\"Digit\")\n",
    "plt.ylabel(f\"Number of classifciations (N={len(preds_test)})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_thresh = 0.5\n",
    "\n",
    "xors_train_rounded = outputs.mean(axis=2).copy()\n",
    "xors_train_rounded[xors_train_rounded <= xor_thresh] = 0\n",
    "xors_train_rounded[xors_train_rounded > xor_thresh] = 1\n",
    "xor_train_actual = np.array(\n",
    "    [xor(train_samples[:, i]) for i in range(train_samples.shape[1])]\n",
    ")\n",
    "\n",
    "\n",
    "mask_0 = np.isclose(xors_train_rounded, 0).squeeze()\n",
    "mask_1 = np.isclose(xors_train_rounded, 1).squeeze()\n",
    "plt.scatter(\n",
    "    train_samples[0, mask_0],\n",
    "    train_samples[1, mask_0],\n",
    "    c=\"red\",\n",
    "    marker=\"x\",\n",
    "    label=\"XOR = 0\",\n",
    ")\n",
    "plt.scatter(\n",
    "    train_samples[0, mask_1],\n",
    "    train_samples[1, mask_1],\n",
    "    c=\"green\",\n",
    "    marker=\"o\",\n",
    "    label=\"XOR = 1\",\n",
    ")\n",
    "\n",
    "mask_incorrect = (xors_train_rounded != xor_train_actual).squeeze()\n",
    "plt.scatter(\n",
    "    train_samples[0, mask_incorrect],\n",
    "    train_samples[1, mask_incorrect],\n",
    "    marker=\"+\",\n",
    "    c=\"yellow\",\n",
    ")\n",
    "\n",
    "plt.axis(\"equal\")\n",
    "plt.axvline(x=0, c=\"black\")\n",
    "plt.axhline(y=0, c=\"black\")\n",
    "\n",
    "plt.title(\n",
    "    f\"Network Approximation of XOR function  (Applied to Training Data)\\nAccuracy = {100 * (1 - sum(mask_incorrect) / num_samples_train)}%\"\n",
    ")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Test XOR Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_test = infer(test_samples, neurons, decoders, config)\n",
    "#  (D x S) x (D x Y).T @ (D x S) ==> Y x\n",
    "plt.hist(outputs_test.mean(axis=2).T, 20, density=True)\n",
    "plt.title(f\"Test Eval Output Probability Histogram N={test_samples.shape[1]}\")\n",
    "plt.xlabel(\"P(XOR Evals to True)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_thresh = 0.5\n",
    "\n",
    "xors_test_rounded = outputs_test.mean(axis=2).copy()\n",
    "xors_test_rounded[xors_test_rounded <= xor_thresh] = 0\n",
    "xors_test_rounded[xors_test_rounded > xor_thresh] = 1\n",
    "xors_test_actual = np.array(\n",
    "    [xor(test_samples[:, i]) for i in range(test_samples.shape[1])]\n",
    ")\n",
    "\n",
    "\n",
    "mask_0 = np.isclose(xors_test_rounded, 0).squeeze()\n",
    "mask_1 = np.isclose(xors_test_rounded, 1).squeeze()\n",
    "plt.scatter(\n",
    "    test_samples[0, mask_0],\n",
    "    test_samples[1, mask_0],\n",
    "    c=\"red\",\n",
    "    marker=\"x\",\n",
    "    label=\"XOR = 0\",\n",
    ")\n",
    "plt.scatter(\n",
    "    test_samples[0, mask_1],\n",
    "    test_samples[1, mask_1],\n",
    "    c=\"green\",\n",
    "    marker=\"o\",\n",
    "    label=\"XOR = 1\",\n",
    ")\n",
    "\n",
    "mask_incorrect = (xors_test_rounded != xors_test_actual).squeeze()\n",
    "plt.scatter(\n",
    "    test_samples[0, mask_incorrect],\n",
    "    test_samples[1, mask_incorrect],\n",
    "    marker=\"+\",\n",
    "    c=\"yellow\",\n",
    ")\n",
    "\n",
    "plt.axis(\"equal\")\n",
    "plt.axvline(x=0, c=\"black\")\n",
    "plt.axhline(y=0, c=\"black\")\n",
    "\n",
    "plt.title(\n",
    "    f\"Network Approximation of XOR function  (Applied to Test Data)\\nAccuracy = {100 * (1 - sum(mask_incorrect) / num_samples_train)}%\"\n",
    ")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
